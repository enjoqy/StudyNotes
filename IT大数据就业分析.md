# IT大数据就业分析

整体流程：

- 项目的背景（找到问题，解决问题）
  - 当下存在的一些问题
  - 自己项目能解决那些问题
  - 那些技术的出现为自己项目的实现提供了可能
- 介绍自己项目的平台以及面向的用户群
- 介绍自己项目所用的技术和技术的优势
- 介绍自己的开发团队
  - 团队成员分别是哪些角色，每人做什么
- 介绍自己产品的亮点
- 介绍产品的框架
  - 怎么设计的
  - 哪些流程
  - 用什么框架搭建的
- 产品实际展示，或者展示截图、视频
- 致谢
  - 感谢那些为项目负责的团队，个人、老师、平台提供者

## 1、软件环境

- 前端

  - html、css、js、jquery、ecahrs、ajax

- 后端

  - jsp、SpringBoot

  ​	

## 2、开发工具

- Eclipse、IDEA、MySQL、Tomcat8.5、JDK1.8、Git、Maven
- Pycharm、Python3.6、Scrapy

##  3、收集数据

- 系统环境
  
  - win10、 Python3.6
  
- 开发工具
  
  - Pycharm
  
- 技术
  
  - Scrapy框架、
  
- 反爬虫
  
  - 增加请求头
  - 随机延时
  - ip代理池
  - 模拟登陆
  - 账号池
  - 模拟浏览器
  - 验证码
- 爬虫道德节操和法律问题
  - robot.txt
  - 控制采集速度
  - 注意商业用途

## 4、就业大数据智能分析平台

- 就业大数据智能分析平台
- 面向人群
  - 各大高校毕业后求职的毕业生
  - 想跳槽it行业的从业人员
  - 像我们北大青鸟这样的培训机构
- 主要获取各大招聘网站的热门it岗位的信息，进行分析和处理，将数据更加直观和明了的展现给用户
- tableau工具  
  - 报表分析工具
  - 桌面端软件Desktop
  - 企业内部数据共享的服务器端Server
  - 帮助没有IT基础的人们将数据应用于视觉化思考
  - 支持现有主流的各种数据源类型
    - 逗号分隔文本文件
    - Web数据源
    - 关系数据库和多维数据库。
    - Microsoft Office 文件
- 同时进行线上和线下的宣传，从而吸引大量的毕业生和那些想从事it行业的从业者
- 项目流程：

  - 第一步：使用python的scrapy爬虫框架进行数据的获取
  - 第二步：使用hadoop集群对数据进行采集和清洗

  - 最后一步：对数据的可视化
    - 我们的可视化分为两个部分
      - 第一部分：tableau工具进行线下的大屏数据展示
      - 第二部分：web端的Echarts实现图表的数据展示
- 所有的数据都是用ajax从后端动态获取的，后端使用SpringBoot，完成了前后端分离的开发
- web端
  - 布局整体采用了adminLTE模板，
  - 使用了bootstrap风格，
  - 具有响应式的功能，
  - 可以同时兼容pc端和移动端，在移动端是布局是不会受影响的，
  - 对用户的体验是非常友好的
  - 减轻pc端的服务器的压力
- Hadoop
  - hadoop集群
  - hdfs存储真实数据
  - hive存的是数据的hdfs中的地址
  - yarn是hadoop与hive之间的资源调度管理
  - mysql：作为hive的元数据库
- 为什么使用hadoop
  - 生产环境中数据量庞大，存mysql数据库效率低
  - 使用文本数据存放到hdfs系统中
- 优化MapReduce
  - 修改hive数据仓库的参数



